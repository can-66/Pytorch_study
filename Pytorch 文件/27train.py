from torch import nn
import torch
from torch.utils.data import DataLoader
import torchvision
from model import TuiDui#引入模型·,一定要是在同一文件夹下的
from torch.utils.tensorboard import SummaryWriter
#准备数据集
train_data=torchvision.datasets.CIFAR10(root="./dataset",train=True,
                                         transform=torchvision.transforms.ToTensor(),download=True)
test_data=torchvision.datasets.CIFAR10(root="./dataset",train=False,
                                         transform=torchvision.transforms.ToTensor(),download=True)
#length长度
train_data_length=len(train_data)
test_data_length=len(test_data)
print("训练数据集长度：{}".format(train_data_length))#训练数据集长度：50000
print("测试数据集长度：{}".format(test_data_length))#测试数据集长度：10000

#利用DataLoader加载数据集
train_dataloader=DataLoader(train_data,batch_size=64,shuffle=True)
test_dataloader=DataLoader(test_data,batch_size=64,shuffle=True)

# #创建网络模型
# class TuiDui(nn.Module):
#     def __init__(self):
#         super().__init__()
#         self.model=nn.Sequential(
#             nn.Conv2d(3,32,5,1,2),
#             nn.MaxPool2d(2),
#             nn.Conv2d(32,32,5,1,2),
#             nn.MaxPool2d(2),
#             nn.Conv2d(32,64,5,1,2),
#             nn.MaxPool2d(2),
#             nn.Flatten(),
#             nn.Linear(64*4*4,64),
#             nn.Linear(64,10)
#         )
    
#     def forward(self,x):
#         return self.model(x)

#创建网络模型
tuidui=TuiDui()

#损失函数
loss_fn=nn.CrossEntropyLoss()

#优化器
learning_rate=1e-2
optimizer=torch.optim.SGD(tuidui.parameters(),lr=learning_rate)


#设置训练网络的一些参数

#记录训练次数
total_train_step=0
#记录测试的次数
total_test_step=0
#训练的轮数
epoch=10

#添加tensorboard
writer=SummaryWriter('./new_logs_train')

tuidui.train()
for i in range(epoch):
    print("------------第{}轮训练开始-------------".format(i+1))
    #训练开始
    for data in train_dataloader:
        imgs,targets=data
        outputs=tuidui(imgs)
        print(outputs)
        loss=loss_fn(outputs,targets)

        #优化器优化模型
        optimizer.zero_grad()#梯度清零
        loss.backward()
        optimizer.step()

        total_train_step+=1
        if total_train_step % 100==0:
            print("训练次数：{},loss: {}".format(total_train_step,loss.item()))
            writer.add_scalar("train_loss",loss.item(),total_train_step)
        
        
    #测试步骤开始
    #在“评估阶段”计算整套数据的损失总和
    #输出准确率
    total_test_loss=0
    total_accuracy=0




    tuidui.eval()
    with torch.no_grad():
        #用了 with torch.no_grad()，表示推理阶段不建计算图，节省显存/加速；
        for data in test_dataloader:
            imgs,targets=data
            outputs=tuidui(imgs)
            
            loss=loss_fn(outputs,targets)
            total_test_loss+=loss.item()
            accuracy=(outputs.argmax(1)==targets).sum()
            #argmax(1)：每行取最大 → 得到长度为 N 的类别索引（正确用法）
            #形状 [N, C] 的 logits（每行一个样本，每列一个类别分数）。打印出来每行有 10 个数，说明 C=10。
            # 第 0 维是样本数 N，第 1 维是类别数 C。要为每个样本选取“分数最大的类别”，就需要在类别维上取最大值的索引，所以用 argmax(dim=1)。
    #         tensor([[ 1.8172e-02, -8.6195e-02, -7.9255e-02, -4.9410e-02, -7.7612e-02,
    #       3.3355e-02, -8.9875e-03, -1.1207e-01,  5.9761e-02,  2.4615e-02],
    #     [-3.8247e-03, -8.7694e-02, -9.0778e-02, -3.8200e-02, -7.9414e-02,
    #       4.5926e-02,  1.3575e-03, -8.5430e-02,  5.5375e-02,  7.2671e-03],
    #     [ 4.8205e-03, -6.0315e-02, -7.7621e-02, -8.2057e-02, -1.0919e-01,
    #       3.3761e-02,  3.2082e-02, -1.0096e-01,  5.8164e-02,  3.4468e-02],
    #     [ 1.3484e-02, -8.2740e-02, -7.9367e-02, -7.6710e-02, -1.1679e-01,
    #       4.0679e-02, -2.2069e-02, -1.0367e-01,  7.0259e-02,  1.1960e-02],
    #     [-2.6903e-02, -8.3729e-02, -7.4186e-02, -5.6812e-02, -6.8599e-02,
    #       5.4263e-02,  1.7330e-02, -9.1253e-02,  5.4944e-02,  9.2994e-03],
    #     [-9.1313e-03, -9.5117e-02, -7.1408e-02, -4.1265e-02, -8.0233e-02,
    #       4.6575e-02, -3.9587e-03, -8.1804e-02,  6.3138e-02,  3.4115e-03],
    #     [-2.5677e-02, -8.2510e-02, -8.2635e-02, -4.4286e-02, -6.7863e-02,
    #       4.9380e-02,  1.0856e-02, -7.8036e-02,  6.2147e-02,  5.3970e-03],
    #     [ 1.3882e-02, -1.0728e-01, -9.5668e-02, -5.5573e-02, -8.4425e-02,
    #       3.4691e-02, -3.3822e-02, -8.7789e-02,  7.5478e-02,  6.8970e-03],
    #     [-1.3229e-02, -9.4592e-02, -8.2417e-02, -4.2627e-02, -6.0498e-02,
    #       4.9269e-02,  4.6431e-03, -8.7749e-02,  6.9153e-02,  7.7420e-03],
    #     [ 2.2624e-02, -8.2893e-02, -9.9060e-02, -7.6501e-02, -1.0981e-01,
    #       4.4634e-02,  1.1513e-02, -9.5910e-02,  5.8397e-02,  4.0270e-02],
    #     [-1.6942e-02, -9.2202e-02, -7.5447e-02, -4.1638e-02, -5.7395e-02,
    #       5.2423e-02, -7.4069e-03, -7.2089e-02,  5.8799e-02, -6.3142e-03],
    #     [ 9.8568e-03, -9.8857e-02, -7.8987e-02, -3.6783e-02, -6.3250e-02,
    #       3.6690e-02, -2.7207e-02, -9.9414e-02,  5.3744e-02,  7.9697e-04],
    #     [-8.9931e-03, -7.6758e-02, -6.9887e-02, -5.2788e-02, -8.0500e-02,
    #       4.0564e-02,  1.0823e-02, -9.3113e-02,  5.3176e-02,  2.1124e-02],
    #     [ 2.8823e-02, -1.0649e-01, -1.0078e-01, -5.1978e-02, -9.7808e-02,
    #       3.5911e-02, -2.2449e-02, -9.4536e-02,  7.5551e-02,  4.7105e-03],
    #     [-4.6846e-03, -7.3777e-02, -6.9736e-02, -4.5512e-02, -8.0262e-02,
    #       4.5682e-02, -4.3029e-03, -8.0955e-02,  6.0522e-02,  1.2371e-02],
    #     [-1.1491e-02, -8.6581e-02, -6.6358e-02, -5.6442e-02, -8.0513e-02,
    #       4.3450e-02, -1.0563e-02, -8.3591e-02,  7.4694e-02,  1.5468e-02],
    #     [ 1.1700e-02, -8.8731e-02, -8.6705e-02, -5.1679e-02, -8.5824e-02,
    #       4.0116e-02,  1.3159e-03, -8.7176e-02,  6.6114e-02,  1.4277e-02],
    #     [ 2.2664e-02, -8.8419e-02, -7.2701e-02, -7.0187e-02, -1.0097e-01,
    #       1.9491e-02, -2.2725e-02, -9.3743e-02,  7.2871e-02,  1.6128e-02],
    #     [-2.0682e-02, -7.2270e-02, -6.9890e-02, -5.6118e-02, -9.1455e-02,
    #       3.3894e-02,  1.2261e-02, -8.3531e-02,  5.7901e-02,  1.1437e-02],
    #     [ 1.0291e-02, -9.6696e-02, -5.7368e-02, -6.0569e-02, -9.5673e-02,
    #       3.3830e-02, -1.5005e-02, -9.2487e-02,  5.4172e-02,  1.2147e-02],
    #     [ 2.5197e-02, -7.9936e-02, -7.0360e-02, -6.8714e-02, -1.1943e-01,
    #       3.2605e-02, -1.3720e-02, -9.2139e-02,  5.8718e-02,  1.9649e-02],
    #     [-1.6812e-02, -8.2301e-02, -7.9292e-02, -3.0936e-02, -6.3054e-02,
    #       4.5748e-02,  1.0370e-02, -8.6409e-02,  5.6218e-02,  1.3398e-03],
    #     [ 7.3400e-03, -7.6935e-02, -7.2916e-02, -3.3065e-02, -7.5535e-02,
    #       4.4394e-02, -1.8843e-03, -7.9179e-02,  6.7205e-02,  2.1094e-02],
    #     [ 6.2843e-03, -9.2121e-02, -7.2634e-02, -3.8011e-02, -7.6518e-02,
    #       3.6659e-02, -1.8357e-02, -9.0141e-02,  6.7165e-02, -9.1151e-04],
    #     [-2.0412e-03, -8.4850e-02, -6.6350e-02, -3.9035e-02, -9.5848e-02,
    #       3.1452e-02, -2.5058e-02, -8.2517e-02,  5.1141e-02,  9.3209e-03],
    #     [-2.7587e-02, -6.4484e-02, -6.0510e-02, -5.0431e-02, -8.9939e-02,
    #       4.2706e-02,  1.3116e-02, -7.4440e-02,  5.2723e-02,  7.6880e-03],
    #     [-5.5939e-03, -8.2261e-02, -6.9131e-02, -5.0314e-02, -8.5193e-02,
    #       3.8718e-02, -9.4339e-05, -8.3403e-02,  6.1888e-02,  5.8974e-03],
    #     [ 1.6483e-02, -8.5761e-02, -7.2895e-02, -7.1786e-02, -1.1991e-01,
    #       3.3228e-02, -1.7644e-03, -8.9340e-02,  8.2626e-02,  2.4503e-02],
    #     [-1.6175e-02, -8.9404e-02, -7.4879e-02, -3.5512e-02, -5.8000e-02,
    #       5.5251e-02,  1.6551e-02, -7.7800e-02,  4.9621e-02,  1.1898e-04],
    #     [ 1.8469e-02, -8.3779e-02, -8.5607e-02, -7.7785e-02, -1.0730e-01,
    #       2.3252e-02, -8.2705e-03, -8.1115e-02,  8.1158e-02,  2.7984e-02],
    #     [-4.4676e-03, -9.3840e-02, -8.5870e-02, -4.5960e-02, -6.3558e-02,
    #       4.3678e-02, -8.1099e-03, -9.1841e-02,  5.4691e-02,  1.9029e-02],
    #     [-3.0061e-02, -8.8088e-02, -7.4513e-02, -5.7622e-02, -6.9404e-02,
    #       4.8187e-02,  2.2659e-02, -8.4910e-02,  4.5149e-02, -3.5063e-03],
    #     [-1.4275e-02, -8.2960e-02, -6.8059e-02, -5.9364e-02, -7.3564e-02,
    #       4.4998e-02,  1.0373e-02, -8.4501e-02,  5.6301e-02,  7.5470e-03],
    #     [ 1.0705e-02, -7.2714e-02, -7.2787e-02, -4.7121e-02, -1.0219e-01,
    #       5.6034e-02, -1.4508e-02, -8.4427e-02,  7.0918e-02,  1.3207e-02],
    #     [ 3.1698e-02, -9.0758e-02, -8.3548e-02, -7.0465e-02, -1.0879e-01,
    #       3.3964e-02, -2.7106e-02, -7.6966e-02,  6.3301e-02,  2.6536e-02],
    #     [ 2.3442e-02, -9.2822e-02, -7.7897e-02, -4.8678e-02, -8.6511e-02,
    #       3.0668e-02, -2.6876e-02, -8.1707e-02,  7.3118e-02,  5.3853e-03],
    #     [-1.2584e-03, -7.0208e-02, -7.3335e-02, -5.6468e-02, -1.0037e-01,
    #       3.8438e-02, -7.6963e-03, -9.8089e-02,  5.3217e-02,  1.0476e-02],
    #     [ 3.9483e-04, -7.9929e-02, -7.4011e-02, -6.2849e-02, -9.8692e-02,
    #       3.6110e-02,  3.6647e-03, -1.0143e-01,  6.1009e-02,  2.4904e-02],
    #     [ 1.7833e-02, -8.0927e-02, -6.7085e-02, -8.3251e-02, -1.2470e-01,
    #       2.0139e-02, -4.7405e-02, -7.9247e-02,  6.4979e-02,  1.1485e-02],
    #     [-2.7872e-02, -9.2377e-02, -8.5035e-02, -6.4463e-02, -8.4513e-02,
    #       5.8774e-02,  3.7415e-03, -9.2777e-02,  5.2077e-02, -1.3168e-02],
    #     [-4.2279e-02, -8.8203e-02, -7.3124e-02, -5.0887e-02, -6.4311e-02,
    #       5.7477e-02,  1.1015e-02, -7.0812e-02,  3.3175e-02, -3.3985e-04],
    #     [-3.9836e-03, -9.5876e-02, -9.4228e-02, -4.6542e-02, -6.7157e-02,
    #       3.8423e-02, -6.8503e-03, -8.2704e-02,  5.2922e-02,  7.4172e-04],
    #     [-2.4569e-02, -7.4150e-02, -7.7492e-02, -6.7941e-02, -9.6305e-02,
    #       4.7808e-02,  6.5619e-03, -7.4849e-02,  8.0724e-02,  4.3967e-03],
    #     [-9.8491e-03, -8.6243e-02, -8.0671e-02, -5.3119e-02, -9.7541e-02,
    #       3.5949e-02, -8.3127e-03, -7.8406e-02,  6.8456e-02,  1.2129e-02],
    #     [-2.1883e-02, -8.7877e-02, -6.3674e-02, -3.2246e-02, -8.1294e-02,
    #       6.0701e-02, -9.1271e-03, -9.0067e-02,  3.7643e-02, -2.8431e-02],
    #     [-2.1212e-03, -9.1500e-02, -7.1124e-02, -5.0797e-02, -7.3879e-02,
    #       4.2662e-02,  1.1073e-03, -9.2973e-02,  6.4433e-02,  1.0805e-02],
    #     [ 4.2773e-03, -7.7187e-02, -6.4692e-02, -4.5655e-02, -7.9029e-02,
    #       3.2894e-02, -6.6930e-03, -9.3906e-02,  5.5697e-02,  1.3774e-02],
    #     [-4.2492e-03, -9.6588e-02, -9.0090e-02, -4.6722e-02, -5.7704e-02,
    #       4.6642e-02,  3.8510e-03, -9.1368e-02,  5.1608e-02,  8.6483e-03],
    #     [-1.4520e-02, -9.4230e-02, -7.7725e-02, -4.9697e-02, -8.3633e-02,
    #       4.3382e-02, -8.0686e-04, -8.7476e-02,  7.3596e-02,  2.9427e-04],
    #     [-7.5674e-03, -8.8205e-02, -7.7996e-02, -4.8423e-02, -7.5254e-02,
    #       3.9321e-02,  4.3014e-03, -8.7004e-02,  5.7919e-02,  1.4087e-02],
    #     [-4.0292e-02, -9.1681e-02, -8.4841e-02, -6.0920e-02, -9.6767e-02,
    #       6.6665e-02,  4.6206e-03, -8.2115e-02,  7.7220e-02, -2.1926e-03],
    #     [-8.9146e-03, -8.3881e-02, -6.7691e-02, -2.5761e-02, -3.3058e-02,
    #       4.7348e-02,  1.2640e-02, -7.4546e-02,  6.1387e-02,  1.5257e-02],
    #     [-1.2120e-02, -8.3232e-02, -7.4044e-02, -6.7835e-02, -8.5511e-02,
    #       4.2953e-02, -1.0848e-02, -8.3063e-02,  6.5501e-02,  3.1118e-04],
    #     [-6.7457e-03, -7.7467e-02, -8.4902e-02, -6.2900e-02, -1.0197e-01,
    #       5.1471e-02, -2.0710e-03, -9.3517e-02,  7.5040e-02,  1.3950e-03],
    #     [ 8.5313e-03, -7.5765e-02, -6.8537e-02, -6.7441e-02, -1.0050e-01,
    #       2.8056e-02,  8.0050e-03, -7.7772e-02,  5.4862e-02,  3.3012e-02],
    #     [-3.3785e-02, -8.3169e-02, -6.3276e-02, -5.0801e-02, -7.5639e-02,
    #       3.9769e-02,  2.1042e-02, -8.1714e-02,  5.8123e-02,  3.6574e-03],
    #     [-1.5248e-02, -7.7727e-02, -7.6958e-02, -4.3070e-02, -7.1917e-02,
    #       4.4575e-02,  5.5040e-04, -8.7902e-02,  4.8696e-02,  4.3005e-03],
    #     [ 5.2942e-03, -7.3802e-02, -8.2789e-02, -6.3036e-02, -9.8754e-02,
    #       3.9413e-02,  4.7307e-03, -8.6716e-02,  7.3748e-02,  3.0370e-02],
    #     [ 2.8246e-03, -8.4507e-02, -7.9016e-02, -4.5509e-02, -7.7543e-02,
    #       2.0899e-02, -1.3973e-02, -8.5814e-02,  4.7291e-02,  1.3889e-02],
    #     [-1.6019e-02, -9.0722e-02, -7.7331e-02, -5.7936e-02, -8.4499e-02,
    #       3.8576e-02, -4.7088e-03, -7.9270e-02,  6.7999e-02,  9.3421e-03],
    #     [ 1.3225e-02, -9.5931e-02, -7.9638e-02, -4.2426e-02, -7.7441e-02,
    #       4.0021e-02, -2.0137e-02, -8.4689e-02,  5.5636e-02, -4.6119e-03],
    #     [-3.5396e-03, -8.9270e-02, -7.4992e-02, -4.2931e-02, -9.0591e-02,
    #       4.6695e-02,  3.5933e-03, -8.1557e-02,  5.5519e-02,  1.0136e-03],
    #     [ 1.6809e-02, -8.7310e-02, -7.3183e-02, -7.6841e-02, -1.2519e-01,
    #       2.5951e-02, -2.1493e-02, -8.2276e-02,  8.8285e-02,  1.6960e-02],
    #     [ 2.3483e-02, -1.0521e-01, -8.6877e-02, -7.5997e-02, -1.0265e-01,
    #       4.9336e-02, -7.7536e-03, -8.7759e-02,  7.8562e-02,  2.3569e-02]],
    #    grad_fn=<AddmmBackward0>)
            total_accuracy+=accuracy
        
    print("整体测试集上的Loss:{}".format(total_test_loss))
    print("整体测试集上的正确率 :{}".format(total_accuracy/test_data_length))
    writer.add_scalar("test_loss",total_test_loss,total_test_step)
    writer.add_scalar("test_accuracy",total_accuracy/test_data_length,total_test_step)
    total_test_step+=1

    #保存每轮训练后的结果
    torch.save(tuidui,"tuidui{}.path".format(i+1))
    print("模型已保存")


writer.close()

''' 
argmax 是什么
返回最大值所在位置的索引（index）。在分类里常用来把 logits 转成“预测类别”。
torch.argmax(input, dim=None, keepdim=False)
input: 张量
dim: 沿哪一维取最大值的索引
分类输出形状一般是 [N, C]，取 dim=1 得到每个样本的类别索引
keepdim: 是否保留被约简的维度
示例
import torch

logits = torch.tensor([[0.1, 2.3, 0.5],
                       [1.2, 0.7, 3.4]])  # 形状 [N=2, C=3]

pred = torch.argmax(logits, dim=1)   # tensor([1, 2]) -> 每行最大值的位置
dim=0时竖着看
'''



''' 
model.train()：进入训练模式，启用“训练时行为”的层。
model.eval()：进入评估/推理模式，关闭这些训练时行为。
不写也“能跑”，但数值会偏差或不稳定，尤其在验证/测试阶段。
受影响的典型层：
Dropout：train() 随机置零；eval() 关闭随机置零。
BatchNorm：train() 用当前批次均值/方差并更新运行统计；eval() 固定使用历史运行均值/方差，不再更新。
其他层一般不受影响，参数仍会参与前向/反向。
常见问题与影响：
评估时不设 eval()：Dropout 仍随机、BN 用批统计 → 指标抖动且偏低。
训练时误设 eval()：Dropout被关、BN不更新 → 训练难收敛，部署不匹配。
注意：eval() 只切换模式，不会关闭梯度；是否构建计算图由 with torch.no_grad() 决定。

# 训练
model.train()
for imgs, targets in train_loader:
    optimizer.zero_grad(set_to_none=True)
    logits = model(imgs)
    loss = criterion(logits, targets)
    loss.backward()
    optimizer.step()

# 验证/测试
model.eval()
total_loss, total_correct, total = 0.0, 0, 0
with torch.no_grad():
    for imgs, targets in test_loader:
        logits = model(imgs)
        loss = criterion(logits, targets)
        total_loss += loss.item()
        total_correct += (logits.argmax(1) == targets).sum().item()
        total += targets.size(0)
print("val_loss=", total_loss/len(test_loader), "acc=", total_correct/total)
model.train()  # 验证后记得切回训练
'''

