卷积（Convolution）的作用
学习特征：用可学习的卷积核在局部感受野内提取边缘、纹理、形状等模式，逐层形成高层语义。
参数共享：同一卷积核在整张图上滑动，参数更少、泛化更好。
空间结构保留：输出仍是特征图，保留位置信息；可通过步幅/填充/空洞控制感受野与分辨率。
通道变换：可改变通道数（C→C’），提升特征表达能力。
等变性：对平移有一定等变性，有助于检测相同模式在不同位置的出现。
常见用法与形状变化（NCHW）：
步幅为 1、padding 合理：分辨率基本不变（H×W 保持），通道数按卷积核个数变化。
步幅 > 1：同时完成下采样（降低 H、W）。


池化（Pooling）的作用
下采样：降低特征图空间分辨率（H、W 变小），减少计算与内存。
鲁棒性/不变性：对小幅平移/形变更稳健（尤其是最大池化对强响应更敏感）。
抑制过拟合：信息压缩与聚合，起到正则化效果。
无参数或少参数：最大池化/平均池化一般无可学习参数。
常见类型与形状变化：
最大池化 MaxPool2d：保留窗口内最大值，突出显著激活。
平均池化 AvgPool2d：取窗口均值，平滑特征。
全局平均池化 GAP：将 H×W 聚合为 1×1，常用于连接分类层。


何时用哪一个
卷积：需要“学到什么特征”时；提升表达能力、融合上下文、改变通道数。
池化：需要“降采样与鲁棒性”时；快速缩小特征图、降低计算成本。现代网络也常用“步幅卷积/stride=2”代替池化，或使用全局平均池化替代全连接层前的大量参数。
小结
卷积=“学特征 + 可改变通道/分辨率（可选）”
池化=“降采样 + 更稳健（少或无参数）”
若想一边提特征一边降采样：可用步幅卷积；想仅降采样更简单：用池化。