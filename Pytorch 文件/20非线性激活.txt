非线性激活的作用
引入非线性：若没有激活，多层线性层可等价为一层线性层，模型表达力不足；激活让网络能拟合非线性函数。
提升可分性：让模型学习非线性决策边界，处理复杂模式（如图像/语音/文本）。
逐层抽象：配合卷积/全连接，逐层构造更高层语义特征。
稀疏与稳健：如 ReLU 将负值置 0，带来稀疏表示，一定程度抑制噪声与过拟合。
梯度与收敛：合适的激活（ReLU/GELU/SiLU）缓解梯度消失、训练更稳定；不当激活（饱和的 Sigmoid/Tanh）易梯度小。


常见激活与特点
ReLU：y=max(0,x)，快、稳；可能“死亡 ReLU”。
LeakyReLU/ParamReLU：缓解 ReLU 死亡问题。
GELU/SiLU(Swish)：更平滑，性能常更好，计算稍贵。
Tanh/Sigmoid：有界、易饱和，现代深层网络较少直接用在中间层。


选择建议
通用默认：ReLU（或 SiLU/GELU 在 Transformer/ConvNet 中表现也很好）。
遇到大量零激活：试 LeakyReLU/SiLU/GELU 或调学习率/初始化。
输出层按任务选：二分类用 Sigmoid，多分类用 Softmax，回归常不用激活或用适当约束激活。